# Vector Visualization for LLM Embeddings
## Introduction
Welcome to the UOW-AI-Research/vector-visualization repository!

This project focuses on visualizing high-dimensional embeddings generated by Large Language Models (LLMs). Specifically, we explore how to efficiently access and work with word embeddings using vector stores.

## What Are LLM Embeddings?
Embeddings: These are high-dimensional vectors that represent words, phrases, or documents in a numeric format that computers can understand. LLMs use embeddings to understand and generate text.
Vector Stores: These specialized databases allow us to store and retrieve embeddings effectively. They play a crucial role in enabling fast access to word embeddings for LLMs.

## Why Vector Stores?
1. Efficiency: Working directly with word embeddings can be computationally expensive. Vector stores optimize storage and retrieval, making it more efficient to pass embeddings to and from LLMs.
2. Semantic Understanding: Embeddings help LLMs understand word relationships, similarities, and context. This is essential for tasks like semantic search and building chatbots.
3. Retriever Augmented Generation (RAG): By combining retriever-based queries with generative LLM responses, we can provide accurate and up-to-date information without frequent model retraining.

## Getting Started
1. Install Dependencies: Ensure you have the necessary Python libraries installed. Consider using MongoDB, a vector database, for efficient storage and retrieval.
2. Explore Embeddings: Use tools like t-SNE, a dimensionality reduction technique that helps visualize high-dimensional data in a lower-dimensional space (usually 2D or 3D)
3. Visualize: Consider using tools like the Embedding Projector to visualize your embeddings.

